# ClasificaciÃ³n de datos â€œIRISâ€ mediante RNA


Abstractâ€”The selection of parameters and designing neural networks problem is of great importance when designing classification algorithm. This paper presents presents a graphic way of how to select the ideal parameters, with them to make an approach to the development of machine learning algorithms, specifically the algorithm of monolayer and multilayer neural networks to perform the classification of plants.

Index Termsâ€”clasification, neural network, machine learning, data set, Bias.

1. INTRODUCCIÃ“N

A. Redes Neuronales

El cerebro humano es una fuente de inteligencia natural y una computadora paralela verdaderamente notable. El cerebro procesa informacionÂ´ incompleta obtenida por percepciÃ³n a un ritmo increÃ­blemente rapido.Â´ Las celulasÂ´ nerviosas funcionan unas 106 veces masÂ´ lentamente que las puertas de los circuitos electronicos,Â´ pero el cerebro humano procesa la informacionÂ´ visual y auditiva mucho masÂ´ rapidoÂ´ que las computadoras modernas, InspirandoseÂ´ en el cerebro se han estado explorando las redes neuronales artificiales, se modela el cerebro como un sistema dinamicoÂ´ no lineal de tiempo continuo en arquitec- turas conexionistas que se espera que imiten los mecanismos cerebrales para simular un comportamiento inteligente [1].

Las redes neuronales artificiales, o simplemente redes neuronales (NN), Se han propuesto e investigado bastantes modelos NN en los ultimosÂ´ anos.Ëœ Estos modelos NN se pueden clasificar de acuerdo con varios criterios, como sus metodosÂ´ de aprendizaje (supervisado versus no supervisado), arquitecturas (feedforward vs recurrente), tipos de salida (binario versus continuo), tipos de nodo (uniforme versus hÂ´Ä±brido), implementaciones (software versus hardware), pesos de conexionÂ´ (ajustables versus cableados), operaciones (mo- tivadas biologicamenteÂ´ versus motivadas psicologicamente),Â´ etc. En este caso nos limitamos al modelado de problemas con conjuntos de datos de entrada-salida deseados, por lo que las redes resultantes deben tener parametrosÂ´ ajustables que se actualicen mediante una regla de aprendizaje supervisado.

1) Adaline (LMS): En una implementacionÂ´ fÂ´Ä±sica simple,

las senalesËœ de entrada xi son voltajes y las wi son conduc- tancias de resistencias controlables; la salida de la red es la suma de las corrientes causadas por los voltajes de entrada. El problema es encontrar un conjunto adecuado de conductancias (o pesos) de modo que el comportamiento de entrada-salida del Adaline esteÂ´ cerca de un conjunto de puntos de datos de

![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.001.png)

Fig. 1. Neurona

entrada-salida deseados, la medida de error de un Adaline de salida unicaÂ´ se puede expresar como:

Ep = (tp âˆ’ op)2 (1)

donde tp es la salida de destino y op es la salida real de Adaline. La derivada de Ep con respecto a cada peso wi es:

âˆ‚Ex = âˆ’2(t (2) âˆ‚wi p âˆ’ op)xi

Por lo tanto, para disminuir Ep por descenso de gradiente, la formulaÂ´ de actualizacionÂ´ para wi en el patronÂ´ de entrada-salida es:

âˆ‡pwi = Î·(tp âˆ’ op)xi (3)

Se establece que cuando tp > op se quiere impulsar op aumentando wixi, se debe aumentar wi si xi es positivo y disminuir wi si xi es negativo, se usa un razonamiento similar cuando tp < op, dado que el delta trata de minimizar los errores cuadraticosÂ´ o mÂ´Ä±nimos cuadrados promedio (LMS).

2) Perceptron multicapa backpropagation (MLP): Una

backpropagation MLP, es una red adaptativa cuyos nodos (o neuronas) realizan la misma funcionÂ´ en las senalesËœ en- trantes; esta funcionÂ´ de nodo suele ser un compuesto de la suma ponderada y una funcionÂ´ de activacionÂ´ no lineal diferenciable, tambienÂ´ conocida como funcionÂ´ de transferencia Generalmente dejamos que la funcionÂ´ de nodo para la capa

![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.002.png)

Fig. 2. Funciones de activacionÂ´

de salida sea una suma ponderada sin funciones aplastantes. Esto es equivalente a una situacionÂ´ en la que la funcionÂ´ de activacionÂ´ es una funcionÂ´ de identidad y los nodos de salida de este tipo a menudo se denominan nodos lineales. Las MLP

con backpropagation son las estructuras NN masÂ´ utilizadas para aplicaciones en una amplia gama de areas,Â´ como el reconocimiento de patrones, el procesamiento de senales,Ëœ la compresionÂ´ de datos y el control automatico.Â´

3) Regla de aprendizaje backpropagation: el algoritmo de

backpropagation en cuestionÂ´ utiliza la funcionÂ´ logÂ´Ä±stica como su funcionÂ´ de activacionÂ´ La entrada neta x de un nodo se define como la suma ponderada de las senalesËœ entrantes masÂ´ un terminoÂ´ de bias, para la figura[ 3 ](#_page1_x126.64_y328.70)se tiene:

xÂ¯ = wij xi + wj (4)

i

1

xj = f (xÂ¯j ) = (5)

1 + exp(âˆ’xÂ¯j )

Donde xi es la salida del nodo i ubicado en cualquiera de

![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.003.png)

Fig. 3. Nodo j de algoritmo backpropagation

las capas anteriores, wij es el peso asociado con el enlace que conecta los nodos i y j, y wj es el bias del nodo j. Dado que los pesos wij son en realidad parametrosÂ´ internos asociados con cada nodo j, cambiar los pesos de un nodo alteraraÂ´ el comportamiento del nodo y, a su vez, alteraraÂ´ el comportamiento de todo el MLP de backpropagation. La propagacionÂ´ de errores hacia atras,Â´ tambienÂ´ conocida como backpropagarion (BP) o regla delta generalizada (GDR), una medida de error cuadraticoÂ´ para el p-esimoÂ´ par de entrada- salida se define como:

Ep = (dk âˆ’ xk)2 (6)

k

donde dk es la salida deseada para el nodo k, y xk es la salida real para el nodo k cuando se presenta la parte de entrada del p-esimoÂ´ par de datos. Para encontrar el vector gradiente, se define un terminoÂ´ de error eÂ¯j para el nodo i como:

âˆ‚+Ep (7) ÏµÂ¯i = âˆ‚xÂ¯i

Utilizando la regla de la cadena y sabiendo que wij es el peso de la conexionÂ´ del nodo i al j; y wij es cero si no hay conexionÂ´ directa. Luego, la actualizacionÂ´ de peso wki para el aprendizaje en lÂ´Ä±nea es:

âˆ†wki = âˆ’Î·ÏµiÂ¯xk (8)

donde Î· es una tasa de aprendizaje que afecta la velocidad de convergencia y la estabilidad de los pesos durante el apren- dizaje. La formulaÂ´ de actualizacionÂ´ para el sesgo de cada nodo se puede derivar de manera similar. Para el aprendizaje fuera de lÂ´Ä±nea (por lotes), el peso de la conexionÂ´ Wki se actualiza

solo despuesÂ´ de la presentacionÂ´ de todo el conjunto de datos, o solo despuesÂ´ de una epoca,Â´ vectorialmente hablando:

âˆ†w = âˆ’Î·âˆ‡wE (9)

Donde E = p Ep esto corresponde a una forma de usar la verdadera direccionÂ´ del gradiente basada en todo el con- junto de datos Existen bastantes metodosÂ´ para acelerar el entrenamiento backpropagation de MLP. Algunos de ellos son aplicables al descenso de gradiente de backpropagation general

âˆ†w = Î±âˆ†wprev âˆ’ Î·âˆ‡wE (10)

donde wprev es la cantidad de actualizacionÂ´ anterior, y la constante de impulso Î±, generalmente se establece entre 0,1 y 1. La adicionÂ´ del terminoÂ´ de impulso suaviza la actualizacionÂ´ del peso y tiende a resistir los cambios de peso erraticosÂ´

2. DISTRIBUCIOÂ´ N DE LOS DATOS

Se propone la clasificacionÂ´ del data set IRIS, un conjunto de datos â€popularesâ€ en el campo de inteligencia computacional, compuesto de 150 muestras igualmente distribuidas para 3 tipo de floresIris(Setosa, Versicolor y Virginica) cada muestra con- sta de 4 variables descriptoras mas la clase; dichas variables descriptoras son las dimensiones de los petalosÂ´ y sepalosÂ´ de la flor (ancho y largo) para cada una.

![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.004.jpeg)

Fig. 4. Flores Iris

Antes de instanciar o entrenar una red neuronal, se debe hacer un preprocesamiento de los datos, verificar si las clases estanÂ´ balanceadas, que no haya datos faltantes o datos fuera de tendencia y ademasÂ´ de normalizar las variables. Es necesario distribuir el data set en los conjuntos de entrenamiento, prueba y validacion;Â´ para ello se asignan los porcentajes del 70%, 20% y 10% respectivamente, se eliminan las variables que no se usaran, se separan el data set de IRIS en cada una de sus 3 clases, de cada clase se toma aleatoriamente muestras correspondientes a los porcentajes en cuestionÂ´ y se concatenan dichos elementos generando asÂ´Ä± la distribucionÂ´ de los datos, se normalizan los todos los datos en donde el vector de clase tambienÂ´ fue normalizado, los valores objetivo son 0, 0.5 y 1 para las clases 1,2,3 respectivamente. El conjunto de datos de entrenamiento, prueba y validacionÂ´ se exporta para conservarlo y evitar que por la forma aleatoria en que se extrajo, varÂ´Ä±e de una ejecucionÂ´ a otra evitando asÂ´Ä± una pseudo validacionÂ´ cruzada sin intencion.Â´ En resumen, los datos quedaron asÂ´Ä±:

- 105 muestras para entrenamiento , 70%
- 30 muestras para prueba, 20%
- 15 muestras para validacion,Â´ 10%

4

3. SELECCIOÂ´ N DE DESCRIPTORES

Con los 4 descriptores disponibles se realiza un pair plot que se observa en la figura[ 5,](#_page2_x147.11_y522.96) cada uno de las sub graficasÂ´ permite vislumbrar para cada uno de los descriptores si se puede considerar una variable discriminatoria o no; esta validacionÂ´ se hace de forma visual, analizando si en las diferentes graficaÂ´ de dispersionÂ´ es posible plantear fronteras para la clasificacionÂ´ de las flores en cuestionÂ´ o al mismo tiempo validando las aproximaciones de distribucionÂ´ normal para cada descriptor en particular. Considerando lo anterior, se eligen 2 descriptores, siendo estos X3 y X4 ya que tanto en las curvas de distribucionÂ´ normal como en el plano 2d se observa una agrupacionÂ´ por clases y la posibilidad de establecer fronteras entre las mismas. Una consideracionÂ´ teoricaÂ´ adicional es que las variables no estenÂ´ altamente correlacionadas, en este caso, dichos descrip- tores no cumplen esta condicion,Â´ la cual es mas aplicable cuando se tiene un data set con un numero mas grande de descriptores y se quiere elegir los mas representativos.

![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.005.jpeg)

Fig. 5. Pair Plot de Descriptores

4. RED NEURONAL MONOCAPA

Se plantea una red neuronal monocapa con 3 entradas, 2 descriptores masÂ´ el Bias y una neurona de salida con funcionÂ´ de activacionÂ´ lineal, se plantea un algoritmo de LMS (least mean square) para la clasificacionÂ´ de los datos.

Del entrenamiento de la red se obtienen los siguientes pesos optimos:Â´

W = [0.86827173,0.28559754,âˆ’0.04097809]

1. Errores de Entrenamiento, Prueba y ValidacionÂ´
1) Entrenamiento: Con los parametros,Â´ estructura y datos

definidos se entrena la red neuronal; dada que la salida es lineal, el error se analizoÂ´ de forma lineal (distancia aritmetica),Â´

al probar los datos de entrenamiento por la red ya entrenada se calculoÂ´ un error promedio y al mismo tiempo de forma porcentual computando el numero de aciertos con el total de muestras procesadas.

Errores = 5 = 0.0476

105 ErrorLinealPromedio = 0.0999

2) Prueba: Una vez entrenada la red, inicialmente se real-

iza la prueba de la misma con los datos de prueba obteniendo los siguientes resultados:

Errores = 1 = 0.03333

30 ErrorLinealPromedio = 0.10256

3) Validacion:Â´ Por ultimo,Â´ se valida la red neuronal con el

dataset de validacion.Â´

Errores = 1 = 0.06666

15

ErrorLinealPromedio = 0.081785

![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.006.jpeg)

Fig. 6. Error lineal Promedio

En general los resultados son consistentes en las tres etapas de entrenamiento prueba y validacion,Â´ los errores existentes eran de esperarse, dado que los descriptores tienen fronteras difusas entre las clases 2 y 3, y si observamos el compor- tamiento de estas muestras son las que se presentas los errores de la red neuronal. El error porcentual se mantiene cuasi constante al probar los datos, se considera aceptable, al igual que el error promedio lineal que se mantiene alrededor de 0.1 tal y como se exigioÂ´ en la etapa de entrenamiento.

Con estos datos de error podemos aproximar a una precisionÂ´ en la red de entrenamiento del 95% y en el test de 93%, se comparan los resultados con los que obtienen en [5], en donde utilizan 4 caracterÂ´Ä±sticas ademas,Â´ se utilizan algoritmos ya implementados mediante librerÂ´Ä±as y obtienen una precisionÂ´ de entrenamiento del 97% y de test del 96%, ajustando sus parametrosÂ´ para tener errores solo de la clase 1

2. EleccionÂ´ parametrÂ´ os de entrenamiento

Para el error mÂ´Ä±nimo, se eligioÂ´ un promedio igual a 0.1, esto considerando que las clases de salida estanÂ´ espaciadas a 0.5 por lo que el valor intermedio sera de 0.25, por lo tanto se espera que el error lineal a la salida de la red neuronal sea inferior a 0.25 y asÂ´Ä± establecer una regla clara de decision.Â´ En

6

cuanto a los pesos iniciales se establecen valores aleatorios en- tre cero y uno, iniciar con pesos aleatorios es como se plantea el algoritmo de LMS. El paso, Âµ, se establecioÂ´ en 10âˆ’6, dado que por convencionÂ´ debe ser â€pequenoâ€Ëœ pero no se definede un valor en especifico, un paso mas pequenoËœ hace el proceso de entrenamiento mas lento y un paso mas grande puede conllevar a que el algoritmo no converja. Inicialmente se establecioÂ´ un paso de 10âˆ’5, el algoritmo entrenaba rapidamenteÂ´ pero al parecer convergÂ´Ä±a en mÂ´Ä±nimos locales dado que los resultados no eran consistentes.

5. RED NEURONAL MULTICAPA

Se requiere entrenar una red neuronal multicapa con al- goritmo de backpropagation, con funcionÂ´ de activacionÂ´ sig- moidea en la capa interna, para ello se sigue trabajando con los mismos dos descriptores de la red monocapa y se propone inicialmente una red de 3 entradas (2 descriptores mas el Bias) 2 capas: capa oculta con 3 neuronas y una capa de salida con una neurona, todas con funcionÂ´ de activacionÂ´ lineal, luego se hace una variacionÂ´ en el numero de neuronas de la capa oculta, para poder observar que numero de neuronas es mas optimoÂ´ para la clasificacion.Â´

1. Reporte de Errores
1) Entrenamiento: Pesos Optimos wi

âˆ’0.94210976 âˆ’0.66339048 0.30873974 âˆ’1.40982331 âˆ’1.50625083, 0.57928007] 0.04734081 0.14258605, 0.61171693

Pesos Optimos wk

Wk = [âˆ’0.34924836 âˆ’ 1.014925050.066581490.92504914]

Errores = 6 = 0.05714

105                         ErrorPromedio = 0.01999

2) Prueba:

Errores = 300 = 0.0

3) Validacion:Â´

Errores = 151 = 0.06666 (11)

Para esta red con algoritmo de backpropagation no se habla de un error lineal promedio, ya que la funcionÂ´ de error no es lineal, luego, el desempenoËœ se evaluaÂ´ en funcionÂ´ del numeroÂ´ de aciertos en relacionÂ´ a total de muestras procesadas, o en el caso del error, el numeroÂ´ de desaciertos en relacionÂ´ al numeroÂ´ de muestras procesadas.

2. ComparacionÂ´ de Resultados

Comparando con [6] en donde hacen una variacionÂ´ en el algoritmo de backpropagation referente al numero de neuronas de la capa oculta, modificando entre 4, 10 y 15 neuronas, y modifican la distribucionÂ´ de datos a la hora de hacer el testing donde encuentran el mejor resultado con 15 neuronas, 90 datos de entrenamiento y 60 datos de testing, con un error del 2.5% y nuestra red neuronal con 3 neuronas tiene un error aproximado del 6%, que por mas sencilla que sea, clasifica bien

![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.007.png)

Fig. 7. ValidacionÂ´

6. RED NEURONAL MONOCAPA, 4 DESCRIPTORES

Se propone utilizar los 4 descriptores disponibles en el dataset y comparar con los resultados anteriores. En ese caso se plantea una red monocapa con 5 entradas (4 descriptores mas el Bias) y una neurona de salida con una funcionÂ´ de activacionÂ´ lineal. Los demasÂ´ parametrosÂ´ de la red se mantienen iguales a la red monocapa planteada para resolver el problema con solo dos descriptores.

Aumentar el numero de entradas, para el caso en particular, aumento el tiempo y costo computacional de entrenamiento requiriendo 278304 iteraciones Se obtienen los siguientes pesos optimos:

W = [0.28306235,0.04620028,0.40390665,0.68869979,âˆ’0.1469762]

1. Reporte de Errores
1) Entrenamiento:

Errores = 7 = 0.066666

105

ErrorLinealPromedio = 0.0999

2) Prueba:

Errores = 0 = 0.0

30

ErrorLinealPromedio = 0.09426

7

3) Validacion:Â´

Errores = 1 = 0.06666

15 ErrorLinealPromedio = 0.12862

2. ComparacionÂ´ de Resultados

La mejor comparacionÂ´ se da con la red monocapa de 2 descriptores implementada inicialmente, si bien es cierto que el error de prueba disminuyo, la cantidad de errores totales considerando los tres datasets aumento y el costo computacional practicamenteÂ´ se duplico, pasando de 138263 iteraciones a 278304

7. CONCLUSIONES
- Una consideracionÂ´ adicional al observar las muestras en las que hubo error, entre la red multicapa y la otra red monocapa, los errores (linealmente hablando) son de mayor magnitud, i.e., aunque mantiene bajo el numeroÂ´ total de errores (en terminosÂ´ de aciertos) cuando se equivoca en una muestra lo hace en una magnitud mayor (en terminosÂ´ de error lineal)
- Es necesario realizar demasiada prueba y error para lograr que el algoritmo clasifique de forma correcta, variar los parametros,Â´ modificar valores y mucho tiempo de compilacion.Â´
- Las redes neuronales pueden perfectamente trabajar como clasificadores frente a muchos problemas, en este caso funciono para la clasificacionÂ´ de 3 tipos de planta

REFERENCES

1. Notas del curso : Fundamentos de Inteligencia Computacional, Univer- sidad De Antioquia, 2022 - 1
1. â€UCI Machine Learning Repository: Iris Data Setâ€.[ http://archive.ics. uci.edu/ml/datasets/Iris ](http://archive.ics.uci.edu/ml/datasets/Iris)(accedido el 13 de julio de 2022).
1. R. J. Jyh-Shing, â€Supervised Learning Neural Networksâ€, en Neuro- Fuzzy and soft computing a computational approach to learning and machine intelligence. Massachusetts: Pretience Hall, 1997, pp. 226â€“237.
1. â€Conjunto de datos de la flor de Iris â€” 5:09 min â€” documentacionÂ´ de Cursos de AnalÂ´Ä±tica y Machine Learning -â€. Site not found
   1. GitHub Pages.[ https://jdvelasq.github.io/courses/notebooks/sklearn dataset utilities/2-10 load iris.html ](https://jdvelasq.github.io/courses/notebooks/sklearn_dataset_utilities/2-10_load_iris.html)![](Aspose.Words.500a21ea-670b-49f2-851b-476821802036.008.png)(accedido el 13 de julio de 2022).
1. K. Thirunavukkarasu, A. S. Singh, P. Rai and S. Gupta, â€Classifica- tion of IRIS Dataset using Classification Based KNN Algorithm in Supervised Learning,â€ 2018 4th International Conference on Comput- ing Communication and Automation (ICCCA), 2018, pp. 1-4, doi: 10.1109/CCAA.2018.8777643.
1. Weishui Wan, K. Hirasawa, Jinglu Hu and Chunzhi Jin, â€A new method to prune the neural network,â€ Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, 2000, pp. 449-454 vol.6, doi: 10.1109/IJCNN.2000.859436.
8
